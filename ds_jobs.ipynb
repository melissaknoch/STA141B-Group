{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and Analyzing Data Science Job Postings\n",
    "## STA 141B Final Project\n",
    "\n",
    "### Questions from proposal:\n",
    "\n",
    "- Analyze industry data job postings (data analysts, data scientists, data engineers, etc…)\n",
    "\t- Scrape websites for their data science job listings\n",
    "    - indeed, monster, cybercoders\n",
    "\n",
    "- Identify the requirements (skills, experiences, education, etc…) \n",
    "\n",
    "- Identify salary ranges and benefits.\n",
    "\n",
    "- How does location and the geographic region influence the components we list above?\n",
    "\n",
    "- What kinds of software and technologies are workers in this field using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import requests_cache\n",
    "import time\n",
    "import lxml.html as lx\n",
    "import itertools\n",
    "import sqlite3 as sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_cache.install_cache(\"ds_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_cap = [\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\n",
    "  \"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\n",
    "  \"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\n",
    "  \"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\n",
    "  \"Nebraska\",\"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\n",
    "  \"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\n",
    "  \"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\n",
    "  \"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"]\n",
    "states = [state.lower() for state in states_cap]\n",
    "jobs = ['data+scientist']\n",
    "states = [state.replace(' ', '+') for state in states]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 1, scraping links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_front_page_indeed(base_url):\n",
    "    \"\"\"\n",
    "    Scrape one page of Indeed by getting links to all job postings on the page, and providing the link to the next page\n",
    "    Input: Url of the page you wish to scrape\n",
    "    Output: If a blank string was passed for url, then we exhausted all pages so return None.\n",
    "            If a regular url was passed, then a tuple with the the list of links as the first element, \n",
    "            and the link to the next page as the second element is returned \n",
    "    \"\"\"\n",
    "    if base_url == '':\n",
    "        return None\n",
    "    else:\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()\n",
    "        html = lx.fromstring(response.text)\n",
    "        html.make_links_absolute(base_url)\n",
    "\n",
    "        # get links to each posting\n",
    "        links = []\n",
    "        \n",
    "        sponsered_tags = html.xpath(\"//td[contains(@id, 'resultsCol')]/div/div/a\")\n",
    "        sponsered_links = [tag.attrib['href'] for tag in sponsered_tags][1:]\n",
    "        for link in sponsered_links:\n",
    "            links.append(link)\n",
    "        \n",
    "        reg_tags = html.xpath(\"//td[contains(@id, 'resultsCol')]/div/h2/a\")\n",
    "        reg_links = [link.attrib['href'] for link in reg_tags]\n",
    "        for link in reg_links:\n",
    "            links.append(link)\n",
    "\n",
    "         # get links to the next page\n",
    "        nav_tags = html.xpath(\"//td[contains(@id, 'resultsCol')]/div[contains(@class, 'pagination')]/a/span[contains(@class,'pn')]/span[contains(@class,'np')]/../..\")\n",
    "        if len(nav_tags) == 2:\n",
    "            # previous page, next page --> take next page\n",
    "            next_url = nav_tags[1].attrib['href']\n",
    "        elif len(nav_tags) == 0:\n",
    "            # only one page of search results\n",
    "            next_url = ''\n",
    "        else:\n",
    "            # could be either only previous or only next --> return '' if only previous, return link if next\n",
    "            nav_texts = nav_tags[0].text_content()\n",
    "            if 'Previous' in nav_texts:\n",
    "                next_url = ''\n",
    "            else:\n",
    "                next_url = nav_tags[0].attrib['href']\n",
    "                \n",
    "        # make sure we only have links to job postings and nothing else:\n",
    "        links_copy = links.copy()\n",
    "\n",
    "        for link in links_copy:\n",
    "            if 'forum' in link:\n",
    "                links.remove(link)\n",
    "            elif 'salaries' in link:\n",
    "                links.remove(link)\n",
    "\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "        \n",
    "        return list(set(links)), next_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(first_page_url):\n",
    "    \"\"\"\n",
    "    Get all the links from one search specification.\n",
    "    Input: url of the first page returned from the search\n",
    "    Output: list of all the links to job postings from that search\n",
    "    \"\"\"\n",
    "    # start with scraping the first page\n",
    "    links, next_url = scrape_front_page_indeed(first_page_url)\n",
    "    \n",
    "    # initialize posting list\n",
    "    postings = [link for link in links]\n",
    "    \n",
    "    # recall function until you \n",
    "    count = 0\n",
    "    while count < 11:\n",
    "        new_links, next_url = scrape_front_page_indeed(next_url)\n",
    "        for link in new_links:\n",
    "            postings.append(link)\n",
    "        count += 1\n",
    "    \n",
    "    return list(set(postings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.indeed.com/jobs?q=data+scientist&l=california'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-72b278f3ed7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcomb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmy_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.indeed.com/jobs?q={}&l={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-86540a1ab729>\u001b[0m in \u001b[0;36mget_all_links\u001b[0;34m(first_page_url)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mnew_links\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_front_page_indeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_links\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mpostings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "my_dict = {}\n",
    "for comb in itertools.product(jobs, states):\n",
    "    my_dict[comb] = get_all_links('https://www.indeed.com/jobs?q={}&l={}'.format(comb[0], comb[1]))\n",
    "\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 2, scraping posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_post(url):\n",
    "    \"\"\"\n",
    "    Scrape one job post from Indeed.\n",
    "    Input: url of the page to scrape\n",
    "    Output: dictionary with title of job, company, location, text of the post, and the footer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        html = lx.fromstring(response.text)\n",
    "        html.make_links_absolute\n",
    "    except:\n",
    "        return {'url': url, 'title':'', 'info':'', 'text':'', 'footer':''} \n",
    "        \n",
    "    results = {}\n",
    "    \n",
    "    results['url'] = url\n",
    "    \n",
    "    try:\n",
    "        title = html.xpath(\"//div[contains(@class, 'jobsearch-JobComponent')]//h3[contains(@class, 'jobsearch-JobInfoHeader-title')]\")[0].text_content()\n",
    "        results['job_title'] = title\n",
    "    except:\n",
    "        results['job_title'] = ''\n",
    "    \n",
    "    try:\n",
    "        info = [tag.text_content() for tag in html.xpath(\"//div[contains(@class,'jobsearch-DesktopStickyContainer')]/div\") if tag.text_content() != '']\n",
    "        results['info'] = ' '.join(info)\n",
    "    except:\n",
    "        results['info'] = ''\n",
    "    \n",
    "    try:\n",
    "        text = html.xpath(\"//div[contains(@class, 'jobsearch-JobComponent-description')]\")[0].text_content()\n",
    "        results['text'] = text\n",
    "    except:\n",
    "        results['text'] = ''\n",
    "        \n",
    "    try:\n",
    "        footer = html.xpath(\"//div[contains(@class, 'jobsearch-JobMetadataFooter')]\")[0].text_content()\n",
    "        results['footer'] = footer\n",
    "    except:\n",
    "        results['footer'] = ''\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 3, combine posts into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(key, links):\n",
    "    posts = [scrape_post(link) for link in links]\n",
    "    df = pd.concat([pd.DataFrame(posts)])\n",
    "    df['jobsearchterm'] = key[0]\n",
    "    df['state'] = key[1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for key, value in my_dict.items():\n",
    "    add_df = make_df(key, value)\n",
    "    dfs.append(add_df)\n",
    "    \n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
