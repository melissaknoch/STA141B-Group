{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import requests_cache\n",
    "import time\n",
    "import lxml.html as lx\n",
    "import itertools\n",
    "\n",
    "requests_cache.install_cache(\"new_cache\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\n",
    "  \"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\n",
    "  \"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\n",
    "  \"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\n",
    "  \"Nebraska\",\"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\n",
    "  \"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\n",
    "  \"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\n",
    "  \"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"]\n",
    "states = [state.replace(' ', '+') for state in states]\n",
    "jobs = [\"data+scientist\", \"data+analyst\", \"data+engineer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_front_page_indeed(base_url):\n",
    "    \"\"\"\n",
    "    Scrape one page of Indeed by getting links to all job postings on the page, and providing the link to the next page\n",
    "    Input: Url of the page you wish to scrape\n",
    "    Output: If a blank string was passed for url, then we exhausted all pages so return None.\n",
    "            If a regular url was passed, then a tuple with the the list of links as the first element, \n",
    "            and the link to the next page as the second element is returned \n",
    "    \"\"\"\n",
    "    if base_url == '':\n",
    "        return None\n",
    "    else:\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()\n",
    "        html = lx.fromstring(response.text)\n",
    "        html.make_links_absolute(base_url)\n",
    "\n",
    "        # get links to each posting\n",
    "        links = []\n",
    "        \n",
    "        sponsered_tags = html.xpath(\"//td[contains(@id, 'resultsCol')]/div/div/a\")\n",
    "        sponsered_links = [tag.attrib['href'] for tag in sponsered_tags][1:]\n",
    "        for link in sponsered_links:\n",
    "            links.append(link)\n",
    "        \n",
    "        reg_tags = html.xpath(\"//td[contains(@id, 'resultsCol')]/div/h2/a\")\n",
    "        reg_links = [link.attrib['href'] for link in reg_tags]\n",
    "        for link in reg_links:\n",
    "            links.append(link)\n",
    "\n",
    "        # get links to the next page\n",
    "        nav_tags = html.xpath(\"//td[contains(@id, 'resultsCol')]/div[contains(@class, 'pagination')]/a/span[contains(@class,'pn')]/span[contains(@class,'np')]/../..\")\n",
    "        if len(nav_tags) == 2:\n",
    "            # previous page, next page --> take next page\n",
    "            next_url = nav_tags[1].attrib['href']\n",
    "        elif len(nav_tags) == 0:\n",
    "            # only one page of search results\n",
    "            next_url = ''\n",
    "        else:\n",
    "            # could be either only previous or only next --> return '' if only previous, return link if next\n",
    "            nav_texts = nav_tags[0].text_content()\n",
    "            if 'Previous' in nav_texts:\n",
    "                next_url = ''\n",
    "            else:\n",
    "                next_url = nav_tags[0].attrib['href']\n",
    "\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        return list(set(links)), next_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(first_page_url):\n",
    "    \"\"\"\n",
    "    Get all the links from one search specification.\n",
    "    Input: url of the first page returned from the search\n",
    "    Output: list of all the links to job postings from that search\n",
    "    \"\"\"\n",
    "    # start with scraping the first page\n",
    "    links, next_url = scrape_front_page_indeed(first_page_url)\n",
    "    \n",
    "    # initialize posting list\n",
    "    postings = [link for link in links]\n",
    "    \n",
    "    # recall function until you reach the last page, updating posting list each time\n",
    "    while next_url != '':\n",
    "        new_links, next_url = scrape_front_page_indeed(next_url)\n",
    "        for link in new_links:\n",
    "            postings.append(link)\n",
    "    \n",
    "    return list(set(postings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Document is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fd7240b4ba8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcomb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmy_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.indeed.com/jobs?q={}&l={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-bab2caa6c5db>\u001b[0m in \u001b[0;36mget_all_links\u001b[0;34m(first_page_url)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# recall function until you reach the last page, updating posting list each time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mnext_url\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mnew_links\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_front_page_indeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_links\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpostings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-39db07f7a3bd>\u001b[0m in \u001b[0;36mscrape_front_page_indeed\u001b[0;34m(base_url)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_links_absolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lxml/html/__init__.py\u001b[0m in \u001b[0;36mfromstring\u001b[0;34m(html, base_url, parser, **kw)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0mis_full_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_looks_like_full_html_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_fromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_full_html\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lxml/html/__init__.py\u001b[0m in \u001b[0;36mdocument_fromstring\u001b[0;34m(html, parser, ensure_head_body, **kw)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         raise etree.ParserError(\n\u001b[0;32m--> 765\u001b[0;31m             \"Document is empty\")\n\u001b[0m\u001b[1;32m    766\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_head_body\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Document is empty"
     ]
    }
   ],
   "source": [
    "my_dict = {}\n",
    "for comb in list(itertools.product(jobs, states)):\n",
    "    my_dict[comb] = get_all_links('https://www.indeed.com/jobs?q={}&l={}'.format(comb[0], comb[1]))\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
